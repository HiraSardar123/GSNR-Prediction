# -*- coding: utf-8 -*-
"""Supervised Learning on European Topology 6 Paths Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LjtyidQYZgALAVlxrKZQoH0bFRGgQbCm

# **Supervised Learning on European Topology 6 Paths Dataset**
"""

import pandas as pd
filepath='/content/DataSet_EU_3k_5k.xlsx'
df=pd.read_excel(filepath)

"""Using Linear Regression

"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load the data
filepath = '/content/DataSet_EU_3k_5k.xlsx'
df = pd.read_excel(filepath)

df.head()

# Fill missing values with the mean of each column
df.fillna(df.mean(), inplace=True)

# Select numerical columns for normalization
numerical_cols = df.select_dtypes(include=[float, int]).columns
data = df[numerical_cols].values

target = df['GSNR_1']

# Convert to PyTorch tensor
target_tensor = torch.tensor(target, dtype=torch.float32)

# Calculate the mean and standard deviation, handle zero std
mean = target_tensor.mean(dim=0, keepdim=True)
std = target_tensor.std(dim=0, keepdim=True)
std[std == 0] = 1  # Prevent division by zero

# Perform normalization
normalized_tensor = (target_tensor - mean) / std

# Identify existing GSNR columns
gsnr_columns = [col for col in df.columns if 'GSNR' in col]

# Drop existing GSNR columns
features = df.drop(columns=gsnr_columns)

# Select numerical columns for normalization
numerical_cols = features.select_dtypes(include=[float, int]).columns
features_data = features[numerical_cols].values

# Normalize features
features_mean = features_data.mean(axis=0)
features_std = features_data.std(axis=0)
features_std[features_std == 0] = 1  # Prevent division by zero
normalized_features = (features_data - features_mean) / features_std

# Convert to PyTorch tensor
features_tensor = torch.tensor(normalized_features, dtype=torch.float32)

# Example columns: 'Power' and 'GSNR1'
plt.figure(figsize=(10, 6))
plt.scatter(df['Power_1'], df['GSNR_1'], alpha=0.7)
plt.title('Relationship between Power and GSNR1')
plt.xlabel('Power')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['NLI_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('NLI')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['ASE_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('ASE')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['frequency_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('Frequency')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['No. Spans'], df['GSNR_1'], alpha=0.7)
plt.xlabel('No. Spans')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['Total Distance(m)'], df['GSNR_1'], alpha=0.7)
plt.xlabel('Distance')
plt.ylabel('GSNR')
plt.show()

# Define the model
class LinearRegression(nn.Module):
    def __init__(self, input_size):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_size, 1)

    def forward(self, features_tensor):
        y_pred = self.linear(features_tensor)
        return y_pred

size = features_tensor.shape[1]
model = LinearRegression(size)

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.00001)  # Further reduced learning rate

# Training the model
for epoch in range(50):
    model.train()  # Set the model to training mode
    prediction = model(features_tensor)
    target_tensor = normalized_tensor.view(-1, 1)  # Reshape to [18000, 1]

    loss = criterion(prediction, target_tensor)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print('epoch {}, loss {}'.format(epoch, loss.item()))

# Model evaluation
model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    predictions = model(features_tensor).numpy()
    targets = target_tensor.numpy()
    mse = mean_squared_error(targets, predictions)
    r2 = r2_score(targets, predictions)
    mae = mean_absolute_error(targets, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print(f'Mean Absolute Error: {mae}')

# Fine-tuning the model (Example: adjusting learning rate)
learning_rates = [0.001, 0.0001, 0.00001]
best_model = None
best_mse = float('inf')

for lr in learning_rates:
    model = LinearRegression(size)
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)

    for epoch in range(50):
        model.train()
        prediction = model(features_tensor)
        target_tensor = normalized_tensor.view(-1, 1)

        loss = criterion(prediction, target_tensor)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        predictions = model(features_tensor).numpy()
        mse = mean_squared_error(targets, predictions)

        if mse < best_mse:
            best_mse = mse
            best_model = model

    print(f'Learning Rate: {lr}, Mean Squared Error: {mse}')

print(f'Best Model Learning Rate: {lr}, Mean Squared Error: {best_mse}')

"""Using decision Trees and Random Forests"""

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression
import matplotlib.pyplot as plt

# Load the data
filepath = '/content/DataSet_EU_3k_5k.xlsx'
df = pd.read_excel(filepath)

# Fill missing values with the mean of each column
df.fillna(df.mean(), inplace=True)

# Identify target and features
target = df['GSNR_1']
gsnr_columns = [col for col in df.columns if 'GSNR' in col]
features = df.drop(columns=gsnr_columns)

# Select numerical columns for normalization
numerical_cols = features.select_dtypes(include=[float, int]).columns
features_data = features[numerical_cols].values

# Normalize features
features_mean = features_data.mean(axis=0)
features_std = features_data.std(axis=0)
features_std[features_std == 0] = 1  # Prevent division by zero
normalized_features = (features_data - features_mean) / features_std

# Convert target to tensor
target_tensor = torch.tensor(target.values, dtype=torch.float32)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(normalized_features, target_tensor, test_size=0.2, random_state=42)

# Feature selection
selector = SelectKBest(score_func=f_regression, k=10)
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# Decision Tree model
tree_model = DecisionTreeRegressor()
tree_model.fit(X_train_selected, y_train)
tree_predictions = tree_model.predict(X_test_selected)

# Random Forest model
forest_model = RandomForestRegressor(n_estimators=100, random_state=42)
forest_model.fit(X_train_selected, y_train)
forest_predictions = forest_model.predict(X_test_selected)

# Evaluation function
def evaluate_model(predictions, y_true):
    mse = mean_squared_error(y_true, predictions)
    r2 = r2_score(y_true, predictions)
    mae = mean_absolute_error(y_true, predictions)
    print(f'Mean Squared Error: {mse}')
    print(f'R-squared: {r2}')
    print(f'Mean Absolute Error: {mae}')

# Evaluate Decision Tree
print("Decision Tree Performance:")
evaluate_model(tree_predictions, y_test)

# Evaluate Random Forest
print("\nRandom Forest Performance:")
evaluate_model(forest_predictions, y_test)

# Plot feature importances for Random Forest
importances = forest_model.feature_importances_
indices = selector.get_support(indices=True)
selected_features = [numerical_cols[i] for i in indices]

plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances, align='center')
plt.yticks(range(len(importances)), selected_features)
plt.xlabel('Feature Importance')
plt.title('Feature Importances from Random Forest')
plt.show()