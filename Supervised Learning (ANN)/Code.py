# -*- coding: utf-8 -*-
"""Supervised Learning on European Topology 6 Paths Dataset using ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LjtyidQYZgALAVlxrKZQoH0bFRGgQbCm

# **Supervised Learning on European Topology 6 Paths Dataset**
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load the data
filepath = '/content/DataSet_EU_3k_5k.xlsx'
df = pd.read_excel(filepath)

# Fill missing values with the mean of each column
df.fillna(df.mean(), inplace=True)

# Example columns: 'Power' and 'GSNR1'
plt.figure(figsize=(10, 6))
plt.scatter(df['Power_1'], df['GSNR_1'], alpha=0.7)
plt.title('Relationship between Power and GSNR1')
plt.xlabel('Power')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['NLI_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('NLI')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['ASE_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('ASE')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['frequency_1'], df['GSNR_1'], alpha=0.7)
plt.xlabel('Frequency')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['No. Spans'], df['GSNR_1'], alpha=0.7)
plt.xlabel('No. Spans')
plt.ylabel('GSNR')
plt.show()

plt.scatter(df['Total Distance(m)'], df['GSNR_1'], alpha=0.7)
plt.xlabel('Distance')
plt.ylabel('GSNR')
plt.show()

# Select numerical columns for normalization
numerical_cols = df.select_dtypes(include=[float, int]).columns
data = df[numerical_cols].values

target = df['GSNR_1']

# Convert to PyTorch tensor
target_tensor = torch.tensor(target, dtype=torch.float32)

# Calculate the mean and standard deviation, handle zero std
mean = target_tensor.mean(dim=0, keepdim=True)
std = target_tensor.std(dim=0, keepdim=True)
std[std == 0] = 1  # Prevent division by zero

# Perform normalization
normalized_tensor = (target_tensor - mean) / std

# Identify existing GSNR columns
gsnr_columns = [col for col in df.columns if 'GSNR' in col]

# Drop existing GSNR columns
features = df.drop(columns=gsnr_columns)

# Select numerical columns for normalization
numerical_cols = features.select_dtypes(include=[float, int]).columns
features_data = features[numerical_cols].values

# Normalize features
features_mean = features_data.mean(axis=0)
features_std = features_data.std(axis=0)
features_std[features_std == 0] = 1  # Prevent division by zero
normalized_features = (features_data - features_mean) / features_std

features_tensor = torch.tensor(normalized_features, dtype=torch.float32)

from torch.utils.data import Dataset, DataLoader, random_split

class MyDataset(Dataset):
  def __init__(self, features, target):
    self.features = features
    self.target = target

  def __len__(self):
    return len(self.features)

  def __getitem__(self, idx):
    return self.features[idx], self.target[idx]

dataset = MyDataset(features_tensor,target_tensor)

train_size = int(0.7 * len(dataset))
test_size = int(0.15 * len(dataset))
val_size = len(dataset) - train_size - test_size

train_set , val_set , test_set = random_split(dataset,[train_size,val_size,test_size])

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = DataLoader(val_set, batch_size=32, shuffle=False)
test_loader = DataLoader(test_set, batch_size=32, shuffle=False)

import torch.nn.functional as F

input_dim = features_tensor.shape[1]
output_dim = 1

model1 = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)
model2 = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Sigmoid()
)
model3 = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim)  ,
    nn.ReLU()
)

# Define the loss function and optimizer
criterion1 = nn.MSELoss()
optimizer1 = optim.Adam(model1.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model1.train()
    for inputs, targets in train_loader:
        optimizer1.zero_grad()
        outputs = model1(inputs)
        loss = criterion1(outputs, targets.unsqueeze(1))
        loss.backward()
        optimizer1.step()

    # Validation
    model1.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model1(inputs)
            loss = criterion1(outputs, targets.unsqueeze(1))
            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model1.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model1(inputs)
        loss = criterion1(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

# Define the loss function and optimizer
criterion2 = nn.MSELoss()
optimizer2 = optim.Adam(model2.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model2.train()
    for inputs, targets in train_loader:
        optimizer2.zero_grad()
        outputs = model2(inputs)
        loss = criterion2(outputs, targets.unsqueeze(1))
        loss.backward()
        optimizer2.step()

    # Validation
    model2.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model2(inputs)
            loss = criterion2(outputs, targets.unsqueeze(1))
            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model2.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model2(inputs)
        loss = criterion2(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

# Define the loss function and optimizer
criterion3 = nn.MSELoss()
optimizer3 = optim.Adam(model2.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model3.train()
    for inputs, targets in train_loader:
        optimizer3.zero_grad()
        outputs = model3(inputs)
        loss = criterion3(outputs, targets.unsqueeze(1))
        loss.backward()
        optimizer3.step()

    # Validation
    model3.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model3(inputs)
            loss = criterion3(outputs, targets.unsqueeze(1))
            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model3.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model3(inputs)
        loss = criterion3(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

"""TanH performed better than Sigmoid , which performed better than ReLu.
Applying Regularization techniques to it.

Using regularization along with Adam Optimizer.
"""

model = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Example with Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Regularization parameters
l1_lambda = 0.01
l2_lambda = 0.01

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))

        # L1 Regularization
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss += l1_lambda * l1_norm

        # L2 Regularization
        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
        loss += l2_lambda * l2_norm

        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))

            # L1 Regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            loss += l1_lambda * l1_norm

            # L2 Regularization
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss += l2_lambda * l2_norm

            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

"""Using regularization along with SGD Optimizer."""

model = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# SGD
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Regularization parameters
l1_lambda = 0.01
l2_lambda = 0.01

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))

        # L1 Regularization
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss += l1_lambda * l1_norm

        # L2 Regularization
        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
        loss += l2_lambda * l2_norm

        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))

            # L1 Regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            loss += l1_lambda * l1_norm

            # L2 Regularization
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss += l2_lambda * l2_norm

            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

"""Regularization with RMSprop Optimizer."""

model = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)

# Define the loss function and optimizer
criterion = nn.MSELoss()

optimizer = optim.RMSprop(model.parameters(), lr=0.001)

# Regularization parameters
l1_lambda = 0.01
l2_lambda = 0.01

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))

        # L1 Regularization
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss += l1_lambda * l1_norm

        # L2 Regularization
        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
        loss += l2_lambda * l2_norm

        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))

            # L1 Regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            loss += l1_lambda * l1_norm

            # L2 Regularization
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss += l2_lambda * l2_norm

            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

# Evaluation on the test set
model.eval()
test_loss = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f'Test Loss: {test_loss:.4f}')

"""Thus, Adam Optimizer worked best."""

model = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Example with Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Regularization parameters
l1_lambda = 0.01
l2_lambda = 0.01

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))

        # L1 Regularization
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss += l1_lambda * l1_norm

        # L2 Regularization
        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
        loss += l2_lambda * l2_norm

        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))

            # L1 Regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            loss += l1_lambda * l1_norm

            # L2 Regularization
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss += l2_lambda * l2_norm

            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')
test_loss /= len(test_loader)
test_loss, mse, r2, mae = evaluate_model(model, test_loader, criterion)
print(f'Test Loss: {test_loss:.4f}')
print(f'Mean Squared Error (MSE): {mse:.4f}')
print(f'R-squared (R²): {r2:.4f}')
print(f'Mean Absolute Error (MAE): {mae:.4f}')

!pip install scikit-learn

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

model = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, output_dim),
    nn.Tanh()
)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Example with Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Regularization parameters
l1_lambda = 0.01
l2_lambda = 0.01

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.unsqueeze(1))

        # L1 Regularization
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss += l1_lambda * l1_norm

        # L2 Regularization
        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
        loss += l2_lambda * l2_norm

        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))

            # L1 Regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            loss += l1_lambda * l1_norm

            # L2 Regularization
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss += l2_lambda * l2_norm

            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')

def evaluate_model(model, data_loader, criterion):
    """Evaluates the model on the given data loader."""
    model.eval()
    total_loss = 0
    all_targets = []
    all_predictions = []
    with torch.no_grad():
        for inputs, targets in data_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))
            total_loss += loss.item()
            all_targets.extend(targets.tolist())
            all_predictions.extend(outputs.squeeze(1).tolist())

    avg_loss = total_loss / len(data_loader)
    mse = mean_squared_error(all_targets, all_predictions)
    r2 = r2_score(all_targets, all_predictions)
    mae = mean_absolute_error(all_targets, all_predictions)
    return avg_loss, mse, r2, mae



test_loss, mse, r2, mae = evaluate_model(model, test_loader, criterion)
print(f'Test Loss: {test_loss:.4f}')
print(f'Mean Squared Error (MSE): {mse:.4f}')
print(f'R-squared (R²): {r2:.4f}')
print(f'Mean Absolute Error (MAE): {mae:.4f}')